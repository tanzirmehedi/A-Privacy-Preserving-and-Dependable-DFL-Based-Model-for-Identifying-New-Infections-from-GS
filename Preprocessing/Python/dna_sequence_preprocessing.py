# -*- coding: utf-8 -*-
"""DNA Sequence Preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VBDNj6Vrz8vC7w209NlGOLSarPXmTBrE

## DNA Sequencing Pre-Processing

By: Sk. Tanzir Mehedi

1. Import necessary libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import Image
from sklearn.feature_extraction.text import CountVectorizer

"""2. Load DNA sequence data"""

data = pd.read_table('sequencesLabels.txt')
data.head()

"""3. Load full dataset"""

data

"""4. Load dataset overview"""

Image("DataOverview.JPG")

"""5. Select window size and convert to lowaercase"""

def getKmers(sequences, size=3):
    return [sequences[x:x+size].lower() for x in range(len(sequences) - size + 1)]

"""6. Now convert data sequences into short overlapping k-mers of legth 3 using getKmers function."""

data['words'] = data.apply(lambda x: getKmers(x['sequences']), axis=1)
data = data.drop('sequences', axis=1)

data

"""### Now, coding sequence data is changed to lowercase, split up into all possible k-mer words of length 3 and ready for the next step.

7. Convert the lists of k-mers for each gene into string sentences of words that the count vectorizer can use. We can also make a y variable to hold the class labels.
"""

texts = list(data['words'])
for item in range(len(texts)):
    texts[item] = ' '.join(texts[item])
y_data = data.iloc[:, 0].values

y_data

print(texts[1])

"""8. Now apply the BAG of WORDS using CountVectorizer using NLP"""

cv = CountVectorizer(ngram_range=(2,2))
X = cv.fit_transform(texts)

cv

X

print(X.shape)

"""9. Check class distributation of the dataset"""

data['class'].value_counts().sort_index().plot.bar()

"""10. Dictionary representation of One-hot 2D vector representation of generated DNA sequence words with region size = 2"""

seqs = texts
CHARS = 'ACGT'
CHARS_COUNT = len(CHARS)

maxlen = max(map(len, seqs))
res = np.zeros((len(seqs), CHARS_COUNT * maxlen), dtype=np.uint8)

for si, seq in enumerate(seqs):
    seqlen = len(seq)
    arr = np.chararray((seqlen,), buffer=seq)
    for ii, char in enumerate(CHARS):
        res[si][ii*seqlen:(ii+1)*seqlen][arr == char] = 1

res

res.shape

"""11. Print full 2-D vector"""

np.set_printoptions(threshold=100000000)
print(res)

df = pd.DataFrame(res)
df

"""12. Write the pre-processed dataset on our disk for further steps."""

file = open("sample.txt", "w+")
content = str(res)
file.write(content)
file.close()

"""#TML Implementation """

from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

"""1. Splitting the dataset into the training set and test set"""

X_train, X_test, y_train, y_test = train_test_split(X, y_data, test_size = 0.20, random_state=42)

print(X_train.shape)
print(X_test.shape)

"""2. A multinomial naive Bayes classifier will be created.  I previously did some parameter tuning and found the ngram size of 2 (reflected in the Countvectorizer() instance) and a model alpha of 0.1 did the best (the alpha parameter was determined by grid search previously)"""

classifier = MultinomialNB(alpha=0.1)
classifier.fit(X_train, y_train)

"""3. Define the classifier"""

y_pred = classifier.predict(X_test)

"""4. Check the prformance metrices """

print("Confusion matrix\n")
print(pd.crosstab(pd.Series(y_test, name='Actual'), pd.Series(y_pred, name='Predicted')))
def get_metrics(y_test, y_predicted):
    accuracy = accuracy_score(y_test, y_predicted)
    precision = precision_score(y_test, y_predicted, average='weighted')
    recall = recall_score(y_test, y_predicted, average='weighted')
    f1 = f1_score(y_test, y_predicted, average='weighted')
    return accuracy, precision, recall, f1
accuracy, precision, recall, f1 = get_metrics(y_test, y_pred)
print("Accuracy = %.3f \nPrecision = %.3f \nRecall = %.3f \nF1-Score = %.3f" % (accuracy, precision, recall, f1))