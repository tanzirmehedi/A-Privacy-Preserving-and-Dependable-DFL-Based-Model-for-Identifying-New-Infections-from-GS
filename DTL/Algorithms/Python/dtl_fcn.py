# -*- coding: utf-8 -*-
"""DTL-FCN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ySacFuVxzNCpcSchCQoJE2n3Y6ecBTGt

## Deep Transfer Learning with FCN on DNA Sequence Dataset

By: Sk. Tanzir Mehedi

Importing libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import time
import sklearn
import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow.keras as keras
import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.callbacks import TensorBoard
import warnings
# %matplotlib inline
warnings.filterwarnings('ignore')

"""Importing the Dataset"""

dataset=pd.read_csv('preprocessedDNASequenceDatase.csv')

"""Exploratory Data Analysis"""

dataset.head()

properties = list(dataset.columns.values)
properties.remove('label')
X = dataset[properties]
y = dataset['label']

"""Split Dataset into Training Set and Test Set"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

"""Check the nb classes"""

nb_classes = len(np.unique(np.concatenate((y_train, y_test), axis=0)))
nb_classes

"""Transform the labels from integers to one hot vectors"""

enc = sklearn.preprocessing.OneHotEncoder(categories='auto')
enc.fit(np.concatenate((y_train, y_test), axis=0).reshape(-1, 1))

y_train = enc.transform(y_train.values.reshape(-1, 1)).toarray()
y_test = enc.transform(y_test.values.reshape(-1, 1)).toarray()

"""Save orignal y because later we will use binary"""

y_true = np.argmax(y_test, axis=1)

"""If univariate then add a dimension to make it multivariate with one dimension"""

if len(X_train.shape) == 2: 
        X_train = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))
        X_test = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))
        input_shape = X_train.shape[1:]

"""Making the Model"""

input_layer = keras.layers.Input(input_shape)

# Fully connected layer 1

conv1 = keras.layers.Conv1D(filters=128, kernel_size=8, padding='same')(input_layer)
conv1 = keras.layers.BatchNormalization()(conv1)
conv1 = keras.layers.Activation(activation='relu')(conv1)

# Fully connected layer 2

conv2 = keras.layers.Conv1D(filters=256, kernel_size=5, padding='same')(conv1)
conv2 = keras.layers.BatchNormalization()(conv2)
conv2 = keras.layers.Activation('relu')(conv2)

# Fully connected layer 3

conv3 = keras.layers.Conv1D(128, kernel_size=3,padding='same')(conv2)
conv3 = keras.layers.BatchNormalization()(conv3)
conv3 = keras.layers.Activation('relu')(conv3)

# Gap lager

gap_layer = keras.layers.GlobalAveragePooling1D()(conv3)

# Output layer

output_layer = keras.layers.Dense(nb_classes, activation='softmax')(gap_layer)

model = keras.models.Model(inputs=input_layer, outputs=output_layer)

"""Compile the Model"""

model.compile(loss='categorical_crossentropy', optimizer = keras.optimizers.Adam(), metrics=['accuracy'])
model.summary()

"""Result View with TensorBoard"""

NAME = "FCN on DNA Seeuence Dataset"
tensorboard = TensorBoard(log_dir="logs/{}".format(NAME), histogram_freq = 1, profile_batch = 5)

"""Fitting the model"""

# X_test and y_test are only used to monitor the test loss and NOT for training

mini_batch_size = 64
nb_epochs = 1000

start_time = time.time()
history=model.fit(X_train, y_train, batch_size=mini_batch_size, epochs=nb_epochs, validation_data=(X_test, y_test),callbacks=[tensorboard])
duration = time.time() - start_time

"""Making Predictions"""

start_time = time.time()
y_pred = model.predict(X_test)
duration1 = time.time() - start_time

"""Convert the predicted from binary to integer"""

y_pred = np.argmax(y_pred, axis=1)

"""Evaluating the Algorithm"""

print(confusion_matrix(y_true,y_pred))
print(classification_report(y_true,y_pred))

# Model Accuracy: how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_true, y_pred))

# Model Precision: what percentage of positive tuples are labeled as such?
print("Precision:",metrics.precision_score(y_true, y_pred, average='weighted',labels=np.unique(y_pred)))

# Model Recall: what percentage of positive tuples are labelled as such?
print("Recall:",metrics.recall_score(y_true, y_pred,average='weighted', labels=np.unique(y_pred)))

#Calculate F1 Score
print("F1 Score:",metrics.f1_score(y_true, y_pred, average='weighted', labels=np.unique(y_pred)))

#Calculate Mean Absolute Error
print("Mean Absolute Error:",metrics.mean_absolute_error(y_true, y_pred))

# kappa
print("Cohens kappa:", metrics.cohen_kappa_score(y_true, y_pred))

# ROC AUC
print("ROC AUC:", metrics.roc_auc_score(y_true, y_pred))

#Train time
print('Train Time(s): ',duration) 

#Test time
print('Test Time(s): ',duration1)

# list all data in history
print(history.history.keys())

# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

keras.backend.clear_session()