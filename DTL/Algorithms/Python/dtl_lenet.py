# -*- coding: utf-8 -*-
"""DTL-LeNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mTT_2-NeuXFSPJnvcvcJVu-i4omh2jQg

## Deep Transfer Learning with LeNet on DNA Sequence Dataset

By: Sk. Tanzir Mehedi

Importing libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import time
import sklearn
import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow.keras as keras
import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.callbacks import TensorBoard
import warnings
# %matplotlib inline
warnings.filterwarnings('ignore')

"""Importing the Dataset"""

dataset=pd.read_csv('preprocessedDNASequenceDatase.csv')

"""Exploratory Data Analysis"""

dataset.head()

properties = list(dataset.columns.values)
properties.remove('label')
X = dataset[properties]
y = dataset['label']

"""Split Dataset into Training Set and Test Set"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

"""Check the nb classes"""

nb_classes = len(np.unique(np.concatenate((y_train, y_test), axis=0)))
nb_classes

"""Transform the labels from integers to one hot vectors"""

enc = sklearn.preprocessing.OneHotEncoder(categories='auto')
enc.fit(np.concatenate((y_train, y_test), axis=0).reshape(-1, 1))

y_train = enc.transform(y_train.values.reshape(-1, 1)).toarray()
y_test = enc.transform(y_test.values.reshape(-1, 1)).toarray()

"""Save orignal y because later we will use binary"""

y_true = np.argmax(y_test, axis=1)

"""If univariate then add a dimension to make it multivariate with one dimension"""

if len(X_train.shape) == 2: 
        X_train = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))
        X_test = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))
        input_shape = X_train.shape[1:]

"""Making the Model"""

input_layer = keras.layers.Input(input_shape)
        
conv_1 = keras.layers.Conv1D(filters=5,kernel_size=5,activation='relu', padding='same')(input_layer)
conv_1 = keras.layers.MaxPool1D(pool_size=2)(conv_1)
        
conv_2 = keras.layers.Conv1D(filters=20, kernel_size=5, activation='relu', padding='same')(conv_1)
conv_2 = keras.layers.MaxPool1D(pool_size=4)(conv_2)
        
# here did not mention the number of hidden units in the fully-connected layer
# so I took the lenet
        
flatten_layer = keras.layers.Flatten()(conv_2)
fully_connected_layer = keras.layers.Dense(500,activation='relu')(flatten_layer)
        
output_layer = keras.layers.Dense(nb_classes,activation='softmax')(fully_connected_layer)
        
model = keras.models.Model(inputs=input_layer,outputs=output_layer)

"""Compile the Model"""

model.compile(optimizer=keras.optimizers.Adam(lr=0.01,decay=0.005), loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

"""Result View with TensorBoard"""

NAME = "LeNet on DNA Sequence Dataset"
tensorboard = TensorBoard(log_dir="logs/{}".format(NAME), histogram_freq = 1, profile_batch = 5)

"""Fitting the model"""

# x_val and y_val are only used to monitor the test loss and NOT for training
batch_size = 64
nb_epochs = 1000

mini_batch_size = int(min(X_train.shape[0] / 10, batch_size))

start_time = time.time()

history=model.fit(X_train, y_train, batch_size=mini_batch_size, epochs=nb_epochs, validation_data=(X_test, y_test),callbacks=[tensorboard])

duration = time.time() - start_time

"""Making Predictions"""

start_time = time.time()
y_pred = model.predict(X_test)
duration1 = time.time() - start_time

"""Convert the predicted from binary to integer"""

y_pred = np.argmax(y_pred, axis=1)

"""Evaluating the Algorithm"""

print(confusion_matrix(y_true,y_pred))
print(classification_report(y_true,y_pred))

# Model Accuracy: how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_true, y_pred))

# Model Precision: what percentage of positive tuples are labeled as such?
print("Precision:",metrics.precision_score(y_true, y_pred, average='weighted',labels=np.unique(y_pred)))

# Model Recall: what percentage of positive tuples are labelled as such?
print("Recall:",metrics.recall_score(y_true, y_pred,average='weighted', labels=np.unique(y_pred)))

#Calculate F1 Score
print("F1 Score:",metrics.f1_score(y_true, y_pred, average='weighted', labels=np.unique(y_pred)))

#Calculate Mean Absolute Error
print("Mean Absolute Error:",metrics.mean_absolute_error(y_true, y_pred))

# kappa
print("Cohens kappa:", metrics.cohen_kappa_score(y_true, y_pred))

# ROC AUC
print("ROC AUC:", metrics.roc_auc_score(y_true, y_pred))

#Train time
print('Train Time(s): ',duration) 

#Test time
print('Test Time(s): ',duration1)

# list all data in history
print(history.history.keys())

# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

keras.backend.clear_session()